### **MongoDB的数据模型**

**描述数据模型**

**内嵌** 

内嵌的方式指的是把相关联的数据保存在同一个文档结构之中。MongoDB的文档结构允许一个字 

段或者一个数组内的值作为一个嵌套的文档。 

**引用** 

引用方式通过存储数据引用信息来实现两个不同文档之间的关联,应用程序可以通过解析这些数据引 

用来访问相关数据。

#### **如何选择数据模型** 

思考：是否可以让数据自己选内嵌还是引用，根据使用情况，数据库不定时的进行数据整理（就犹如JVM一样）

**选择内嵌** 

- 1. 数据对象之间有包含关系 ,一般是数据对象之间有一对多或者一对一的关系 。 

- 2. 需要经常一起读取的数据。 

- 3. 有 map-reduce/aggregation 需求的数据放在一起，这些操作都只能操作单个 collection。 

**选择引用:** 

- 1. 当内嵌数据会导致很多数据的重复，并且读性能的优势又不足于覆盖数据重复的弊端 。 

- 2. 需要表达比较复杂的多对多关系的时候 。 

- 3. 大型层次结果数据集 嵌套不要太深。 

**MongoDB** **存储引擎** 

存储引擎是MongoDB的核心组件，负责管理数据如何存储在硬盘和内存上。MongoDB支持的存储引擎有 

MMAPv1 ,WiredTiger和InMemory。InMemory存储引擎用于将数据只存储在内存中，只将少量的元数据 

(meta-data)和诊断日志（Diagnostic）存储到硬盘文件中，由于不需要Disk的IO操作，就能获取所需 

的数据，InMemory存储引擎大幅度降低了数据查询的延迟（Latency）。从mongodb3.2开始默认的存储 

引擎是WiredTiger,3.2版本之前的默认存储引擎是MMAPv1，mongodb4.x版本不再支持MMAPv1存储引 

擎。

###  **WiredTiger存储引擎优势** 

1.文档空间分配方式 
    WiredTiger使用的是BTree存储 MMAPV1 线性存储 需要Padding 
2.并发级别 
    WiredTiger 文档级别锁 MMAPV1引擎使用表级锁 
3.数据压缩 
    snappy (默认) 和 zlib ,相比MMAPV1(无压缩) 空间节省数倍。 
4.内存使用 
    WiredTiger 可以指定内存的使用大小。 
5.Cache使用 
    WT引擎使用了二阶缓存WiredTiger Cache, File System Cache来保证Disk上的数据的最终一致性。
    而MMAPv1 只有journal 日志。

#### **checkpoint流程**

1.对所有的table进行一次checkpoint，每个table的checkpoint的元数据更新至WiredTiger.wt 

2.对WiredTiger.wt进行checkpoint，将该table checkpoint的元数据更新至临时文件 

WiredTiger.turtle.set 

3.将WiredTiger.turtle.set重命名为WiredTiger.turtle。 

4.上述过程如果中间失败，WiredTiger在下次连接初始化时，首先将数据恢复至最新的快照状态，然后根 

据WAL恢复数据，以保证存储可靠性。

**Journaling**

在数据库宕机时 , 为保证 MongoDB 中数据的持久性，MongoDB 使用了 Write Ahead Logging 向磁盘 

上的 journal 文件预先进行写入。除了 journal 日志，MongoDB 还使用检查点（checkpoint）来保证 

数据的一致性，当数据库发生宕机时，我们就需要 checkpoint 和 journal 文件协作完成数据的恢复工 

作。

- 1. 在数据文件中查找上一个检查点的标识符 

- 2. 在 journal 文件中查找标识符对应的记录 

- 3. 重做对应记录之后的全部操作 

### **复制集eplica sets**

#### **什么是复制集** 

复制集是由一组拥有相同数据集的mongod实例做组成的集群。 

复制集是一个集群，它是2台及2台以上的服务器组成，以及复制集成员包括Primary主节点,secondary从 

节点和投票节点。 

复制集提供了数据的冗余备份，并在多个服务器上存储数据副本，提高了数据的可用性,保证数据的安全 

性。

#### **为什么要使用复制集**

1.高可用

​		防止设备（服务器、网络）故障。 

​		提供自动failover 功能。 

​		技术来保证高可用 

2.灾难恢复 

​		当发生故障时，可以从其他节点恢复 用于备份。 

3.功能隔离 

​		我们可以在备节点上执行读操作，减少主节点的压力 

​		比如:用于分析、报表，数据挖掘，系统任务等。

#### **复制集集群架构原理**

​       一个复制集中Primary节点上能够完成读写操作,Secondary节点仅能用于读操作。Primary节点需要记 

录所有改变数据库状态的操作,这些记录保存在 oplog 中,这个文件存储在 local 数据库,各个Secondary 

节点通过此 oplog 来复制数据并应用于本地,保持本地的数据与主节点的一致。oplog 具有幂等性,即无 

论执行几次其结果一致,这个比 mysql 的二进制日志更好用。 



复制集数据同步分为初始化同步和keep复制同步。初始化同步指全量从主节点同步数据，如果Primary 

节点数据量比较大同步时间会比较长。而keep复制指初始化同步过后，节点之间的实时同步一般是增量 

同步。 



**初始化同步有以下两种情况会触发：** 

​	(1) Secondary第一次加入。 

​	(2) Secondary落后的数据量超过了oplog的大小，这样也会被全量复制。

MongoDB的Primary节点选举基于心跳触发。一个复制集N个节点中的任意两个节点维持心跳，每个节 

点维护其他N-1个节点的状态。



**心跳检测：** 

整个集群需要保持一定的通信才能知道哪些节点活着哪些节点挂掉。mongodb节点会向副本集中的其他节点 

每2秒就会发送一次pings包，如果其他节点在10秒钟之内没有返回就标示为不能访问。每个节点内部都会 

维护一个状态映射表，表明当前每个节点是什么角色、日志时间戳等关键信息。如果主节点发现自己无法与 

大部分节点通讯则把自己降级为secondary只读节点。 



**主节点选举触发的时机:**

第一次初始化一个复制集 

Secondary节点权重比Primary节点高时，发起替换选举 

Secondary节点发现集群中没有Primary时，发起选举 

Primary节点不能访问到大部分(Majority)成员时主动降级 



当触发选举时,Secondary节点尝试将自身选举为Primary。主节点选举是一个二阶段过程+多数派协 

议。 

```
第一阶段: 
检测自身是否有被选举的资格 如果符合资格会向其它节点发起本节点是否有选举资格的FreshnessCheck,进行同僚仲裁 
第二阶段: 
发起者向集群中存活节点发送Elect(选举)请求，仲裁者收到请求的节点会执行一系列合法性检查，如果检 查通过，则仲裁者(一个复制集中最多50个节点 其中只有7个具有投票权)给发起者投一票。 
pv0通过30秒选举锁防止一次选举中两次投票。 
pv1使用了terms(一个单调递增的选举计数器)来防止在一次选举中投两次票的情况。 
多数派协议: 
发起者如果获得超过半数的投票，则选举通过，自身成为Primary节点。获得低于半数选票的原因，除了常 见的网络问题外，相同优先级的节点同时通过第一阶段的同僚仲裁并进入第二阶段也是一个原因。因此，当 选票不足时，会sleep[0,1]秒内的随机时间，之后再次尝试选举。
```

**分片的工作原理**

```
分片集群由以下3个服务组成： 
Shards Server: 每个shard由一个或多个mongod进程组成，用于存储数据。 
Router Server: 数据库集群的请求入口，所有请求都通过Router(mongos)进行协调，不需要在应用程 
序添加一个路由选择器，Router(mongos)就是一个请求分发中心它负责把应用程序的请求转发到对应的 Shard服务器上。 
Config Server: 配置服务器。存储所有数据库元信息（路由、分片）的配置。
```

**片键shard key）** 

​       为了在数据集合中分配文档，MongoDB使用分片主键分割集合。

**区块（chunk）** 

​       在一个shard server内部，MongoDB还是会把数据分为chunks，每个chunk代表这个shard 

server内部一部分数据。MongoDB分割分片数据到区块，每一个区块包含基于分片主键的左闭右开的 

区间范围。

**思考**：1.如何做到路节点的集群化。2.分片机器加入，分片规则路由节点动态创建。3.分片机器退出，分片规则路由节点动态重建。4.其他mogon数据节点可以带数据直接进行组队。



### **neo4j 的使用思考**

1. 共识产生价值，共识形成知识，能否用世界语言作为软件开发的Dao共识和数据共识，这样就不用每个软件就不用维护一套自己的Dao了。
2. 能否用自然语言解析程序，将自然语言映射成程序能执行方法与命令
3. mongodb 和 neo4j 进行结合，neo4j变成3维度图，节点可以用hash进行快速比对，每个节的hash要有路标的功能，可以实现图的自我动态优化，热点数据层数少，分热点数据，向下沉。